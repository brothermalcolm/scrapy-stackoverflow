Does multi-head attention mechanism in Transformer model really attention multi important tokens at the same time?/questions/72863797/does-multi-head-attention-mechanism-in-transformer-model-really-attention-multi72863797
